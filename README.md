# ðŸ§  AI for Security & Security for AI

Welcome to the **AI-for-Security-and-Security-for-AI** repository!

This repository curates and organizes cutting-edge research at the intersection of **artificial intelligence (AI) and cybersecurity**, with a strong focus on both offensive threats and defensive applications.
Our goal is to provide a structured, up-to-date collection of papers that support:

- **AI for Security**: Leveraging AI to improve core Security Operation Center (SOC) functions such as threat detection, alert triage, anomaly detection, and incident response automation.
- **Security for AI**: Evaluating AI systems robustness against adversarial attacks such as evasion, poisoning, LLM jailbreaking, and model theft.

Papers are organized by research topic and publication year for easier navigation.

---

## ðŸ“š Table of Contents

1. [Agentic AI and Autonomy](#1-agentic-ai-and-autonomy)
2. [Adversarial Attacks on Network Intrusion Detection Systems (NIDS)](#2-adversarial-attacks-on-network-intrusion-detection-systems-nids)
3. [Adversarial Attacks on Large Language Models (LLMs)](#3-adversarial-attacks-on-large-language-models-llms)
4. [Model Stealing and Extraction](#4-model-stealing-and-extraction)
5. [General Adversarial Machine Learning (AML)](#5-general-adversarial-machine-learning-aml)

---

## 1. Agentic AI and Autonomy
keywords: AI Agent, llama, #agentic, LLM, AI agent in SOC, cybersecurity
### ðŸ“… 2025
- [Transforming cybersecurity with agentic AI to combat emerging cyber threats](https://www.sciencedirect.com/science/article/pii/S0308596125000734): This paper investigates the transformative potential of agentic AI in cybersecurity, specifically addressing how it can enhance practices in response to emerging threats. It aims to explore how agentic AI can transform cybersecurity practices, particularly in addressing new and evolving threats, while also examining the cybersecurity risks associated with its integration. The research explores the possibilities for agentic AI to automate critical tasks within Security Operations Centers (SOCs), such as decision-making, incident response, and threat detection. It also emphasizes the risks associated with AI integration, including the introduction of new vulnerabilities and challenges in managing automated systems, which call for a reassessment of existing cybersecurity frameworks to effectively address these risks.
- [TAGAPT: Toward Automatic Generation of APT Samples With Provenance-Level Granularity](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10948500): Detecting advanced persistent threats (APTs) at a host via data provenance has emerged as a valuable yet challenging task. Compared with attack rule matching, machine learning approaches offer new perspectives for efficiently detecting attacks by leveraging their inherent ability to autonomously learn from data and adapt to dynamic environments. However, the scarcity of APT samples poses a significant limitation, rendering supervised learning methods that have demonstrated remarkable capabilities in other domains (e.g., malware detection) impractical. Therefore, we propose a system called TAGAPT, which is able to automatically generate numerous APT samples with provenance-level granularity. First, we introduce a deep graph generation model to generalize various graph structures that represent new attack patterns. Second, we propose an attack stage division algorithm to divide each generated graph structure into stage subgraphs. Finally, we design a genetic algorithm to find the optimal attack technique explanation for each subgraph and obtain fully instantiated APT samples. Experimental results demonstrate that TAGAPT can learn from existing attack patterns and generalize to novel attack patterns. Furthermore, the generated APT samples 1) exhibit the ability to help with efficient threat hunting and 2) provide additional assistance to the state-of-theart (SOTA) attack detection system (Kairos) by filtering out 73% of the observed false positives. We have open-sourced the code and the generated samples to support the development of the security community
- 
- 
- 
### ðŸ“… 2024
- [Using LLMs as AI Agents to Identify False Positive Alerts in Security Operation Center](https://www.researchsquare.com/article/rs-5420741/v1): This paper addresses the challenges and solutions related to identifying false positive (FP) alerts in Security Information and Event Management (SIEM) systems, which often overwhelm security operators. To tackle this issue, we propose a novel approach that employs a Large Language Model (LLM), specifically Llama, as an AI agent through a contextual-based approach to identify FPs in security alerts generated by multiple network sensors and collected in Security Operations Centers (SOCs). Our method follows three key steps: data extraction, enrichment, and playbook execution. First, Llama normalizes security alerts using a common schema, extracting key contextual elements such as IP addresses, host names, filenames, services, and vulnerabilities. Second, these extracted elements are enriched by integrating external resources such as threat intelligence databases and Configuration Management Databases (CMDB) to generate dynamic metadata. Finally, this enriched data is analyzed through predefined false positive investigation playbooks, designed by security professionals, to systematically evaluate and identify FPs.By automating the false positive identification process, this approach reduces the operational burden on human security operators, enhancing the overall efficiency and accuracy of SOCs, and improving the organization's security posture.
---
## 2. Adversarial Attacks on Network Intrusion Detection Systems (NIDS)

### ðŸ“… 2025
- [Adversarial Evasion Attacks Practicality in Networks: Testing the Impact of Dynamic Learning](https://arxiv.org/pdf/2306.05494)

### ðŸ“… 2024
- [Explainable and Transferable Adversarial Attack for ML-Based Network Intrusion Detectors](https://arxiv.org/pdf/2401.10691#page=17&zoom=100,416,53)
- [Towards Realistic Problem-Space Adversarial Attacks Against ML in NIDS](https://dl.acm.org/doi/pdf/10.1145/3664476.3669974)

### ðŸ“… 2023
- [Black-box Adversarial Example Attack Towards FCG-Based Android Malware Detection](https://www.usenix.org/system/files/sec23fall-prepub-2-li-heng.pdf)
- [Evading Provenance-Based ML Detectors with Adversarial System Actions](https://www.usenix.org/system/files/usenixsecurity23-mukherjee.pdf)
- [A Comprehensive Survey of GANs in Cybersecurity Intrusion Detection](https://ieeexplore.ieee.org/abstract/document/10187144)
- [Survey: AML for Network Intrusion Detection Systems](https://ieeexplore.ieee.org/abstract/document/10005100)
- ["Real Attackers Don't Compute Gradients": Bridging the Gap Between Research and Practice](https://ieeexplore.ieee.org/abstract/document/10136152)

### ðŸ“… Prior
- **2022**: [Practicality of Adversarial Evasion Attacks on NIDS](https://link.springer.com/article/10.1007/s12243-022-00910-1)

---

## 3. Adversarial Attacks on Large Language Models (LLMs)

### ðŸ“… 2024
- [SoK: All You Need to Know About On-Device ML Model Extraction](https://www.usenix.org/system/files/usenixsecurity24-nayan.pdf)

### ðŸ“… 2023
- [Survey of Vulnerabilities in LLMs Revealed by Adversarial Attacks](https://arxiv.org/pdf/2310.10844)
- [A Survey on Transferability of Adversarial Examples Across DNNs](https://arxiv.org/pdf/2310.17626)

---

## 4. Model Stealing and Extraction

### ðŸ“… 2024
<!-- Add new papers here -->

### ðŸ“… 2023
<!-- Add new papers here -->

### ðŸ“… Prior
- **2022**: [Surrogate-Based Black-Box Attacks on Deep Networks](https://arxiv.org/pdf/2203.08725)
- **2021**: [Meta-Surrogate Model for Transferable Adversarial Attack](https://arxiv.org/pdf/2109.01983)

---

## 5. General Adversarial Machine Learning (AML)

### ðŸ“… 2024
- [Benchmarking Transferable Adversarial Attacks](https://arxiv.org/pdf/2402.00418)

### ðŸ“… 2023
<!-- Add papers here -->

### ðŸ“… Prior
- **NeurIPS 2019**: [Adversarial Examples are not Bugs, They are Features](https://proceedings.neurips.cc/paper_files/paper/2019/file/e2c420d928d4bf8ce0ff2ec19b371514-Paper.pdf)
- **CVPR 2019**: [Feature Space Perturbations Yield More Transferable Adversarial Examples](https://openaccess.thecvf.com/content_CVPR_2019/papers/Inkawhich_Feature_Space_Perturbations_Yield_More_Transferable_Adversarial_Examples_CVPR_2019_paper.pdf)

---

