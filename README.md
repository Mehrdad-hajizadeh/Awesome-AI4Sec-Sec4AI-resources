# üß† AI for Security & Security for AI

Welcome to the **AI-for-Security-and-Security-for-AI** resources!

This repository curates and organizes cutting-edge research at the intersection of **artificial intelligence (AI) and cybersecurity**, with a clear separation of two core themes:
### - üõ°Ô∏è AI4Sec (AI for Security): 

AI4Sec focuses on how AI can strengthen cybersecurity operations across the full defensive and offensive spectrum. This includes the use of machine learning and large language models (LLMs) for:

  - üîç Threat detection and alert triage  
  - üìà Anomaly detection and behavioral analysis  
  - ü§ñ Agent-based response and automation  
  - üïµÔ∏è AI-assisted offensive techniques (e.g., reconnaissance, phishing, evasion)

To provide structure and industry alignment, this section follows:

  - The **[NIST Cybersecurity Framework](https://www.nist.gov/cyberframework)** for defensive categories such as Govern, Identify, Protect, Detect, Respond, and Recover.  
  - The **[MITRE ATT&CK Matrix](https://attack.mitre.org/)** to map offensive AI use cases to real-world adversary tactics such as Reconnaissance and Resource Development.

### - üîì Sec4AI (Security for AI): 

Sec4AI focuses on protecting AI systems themselves from evolving cybersecurity threats. This includes understanding and mitigating risks targeting machine learning models, large language models (LLMs), and agentic systems.

Key areas of concern include:

- üéØ Adversarial attacks (evasion, poisoning)
- üß† Model stealing, inversion, and privacy leakage
- üîì Jailbreaking and prompt injection in LLMs
- üì¶ Secure training pipelines and deployment practices

This section is organized using the **[MITRE ATLAS Framework](https://atlas.mitre.org/matrices/ATLAS)**, which systematically categorizes tactics and techniques used by real-world adversaries against AI systems.

---

## üìö Table of Contents

### 1. AI4Sec ‚Äî AI for Security

#### 1.1 Defensive Security ([NIST Cybersecurity Framework](https://www.nist.gov/cyberframework))
- [1.1.1 Govern](#)
- [1.1.2 Identify](#)
- [1.1.3 Protect](#)
- [1.1.4 Detect](#)
- [1.1.5 Respond](#)
- [1.1.6 Recover](#)

#### 1.2 Offensive Security ([MITRE ATT&CK Matrix](https://attack.mitre.org/))
- [1.2.1 Reconnaissance](#)
- [1.2.2 Resource Development](#)
- [1.2.3 Initial Access](#)
- [1.2.4 Execution](#)
- [1.2.5 Persistence](#)
- [1.2.6 Privilege Escalation](#)
- [1.2.7 Defense Evasion](#)
- [1.2.8 Credential Access](#)
- [1.2.9 Discovery](#)
- [1.2.10 Lateral Movement](#)
- [1.2.11 Collection](#)
- [1.2.12 Command and Control](#)
- [1.2.13 Exfiltration](#)
- [1.2.14 Impact](#)

#### 1.3 Misc
- [1.3.1 Dataset](#)
- [1.3.2 Open-source tools](#)

---
### 2. Sec4AI ‚Äî Security for AI
#### 2.1 Defensive Security for AI Models (no standard framework)
- [2.1.1 Detection](#)
- [2.2.1 Mitigation](#)
#### 2.2 Offensive Security ([MITRE ATLAS framework](https://atlas.mitre.org/matrices/ATLAS))
- [2.2.1 Reconnaissance](#)
- [2.2.2 Resource Development](#)
- [2.2.3 Initial Access](#)
- [2.2.4 AI Model Access](#)
- [2.2.5 Execution](#)
- [2.2.6 Persistence](#)
- [2.2.7 Privilege Escalation](#)
- [2.2.8 Defense Evasion](#)
- [2.2.9 Credential Access](#)
- [2.2.10 Discovery](#)
- [2.2.11 Collection](#)
- [2.2.12 AI Attack Staging](#)
- [2.2.13 Command and Control](#)
- [2.2.14 Exfiltration](#)
- [2.2.15 Impact](#)

#### 2.3 Misc
- [2.3.1 Dataset](#)
- [2.3.2 Open-source tools](#)
