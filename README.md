# ðŸ§  AI for Security & Security for AI

Welcome to the **AI-for-Security-and-Security-for-AI** repository!

This repository curates and organizes cutting-edge research at the intersection of **artificial intelligence (AI) and cybersecurity**, with a strong focus on both offensive threats and defensive applications.
Specifically, it highlights how AI can be leveraged to enhance Security Operations Center (SOC) effectiveness â€” including threat detection, alert triage, incident response, and anomaly analysis. It also explores how to secure AI systems themselves against emerging threats like adversarial attacks, model extraction, and misuse of autonomous agents.

Our goal is to provide a structured, up-to-date collection of papers that support:

- **AI for Security**: Leveraging AI to improve core SOC functions such as threat detection, alert triage, anomaly detection, and incident response automation.
- **Security for AI**: Evaluating AI systems robustness against adversarial attacks such as evasion, poisoning, LLM jailbreaking, and model theft.

Papers are organized by research topic and publication year for easier navigation.

---

## ðŸ“š Table of Contents

1. [Agentic AI and Autonomy](#1-agentic-ai-and-autonomy)
2. [Adversarial Attacks on Network Intrusion Detection Systems (NIDS)](#2-adversarial-attacks-on-network-intrusion-detection-systems-nids)
3. [Adversarial Attacks on Large Language Models (LLMs)](#3-adversarial-attacks-on-large-language-models-llms)
4. [Model Stealing and Extraction](#4-model-stealing-and-extraction)
5. [General Adversarial Machine Learning (AML)](#5-general-adversarial-machine-learning-aml)

---

## 1. Agentic AI and Autonomy
### ðŸ“… 2025
- 
### ðŸ“… 2024
---
## 2. Adversarial Attacks on Network Intrusion Detection Systems (NIDS)

### ðŸ“… 2025
- [Adversarial Evasion Attacks Practicality in Networks: Testing the Impact of Dynamic Learning](https://arxiv.org/pdf/2306.05494)

### ðŸ“… 2024
- [Explainable and Transferable Adversarial Attack for ML-Based Network Intrusion Detectors](https://arxiv.org/pdf/2401.10691#page=17&zoom=100,416,53)
- [Towards Realistic Problem-Space Adversarial Attacks Against ML in NIDS](https://dl.acm.org/doi/pdf/10.1145/3664476.3669974)

### ðŸ“… 2023
- [Black-box Adversarial Example Attack Towards FCG-Based Android Malware Detection](https://www.usenix.org/system/files/sec23fall-prepub-2-li-heng.pdf)
- [Evading Provenance-Based ML Detectors with Adversarial System Actions](https://www.usenix.org/system/files/usenixsecurity23-mukherjee.pdf)
- [A Comprehensive Survey of GANs in Cybersecurity Intrusion Detection](https://ieeexplore.ieee.org/abstract/document/10187144)
- [Survey: AML for Network Intrusion Detection Systems](https://ieeexplore.ieee.org/abstract/document/10005100)
- ["Real Attackers Don't Compute Gradients": Bridging the Gap Between Research and Practice](https://ieeexplore.ieee.org/abstract/document/10136152)

### ðŸ“… Prior
- **2022**: [Practicality of Adversarial Evasion Attacks on NIDS](https://link.springer.com/article/10.1007/s12243-022-00910-1)

---

## 3. Adversarial Attacks on Large Language Models (LLMs)

### ðŸ“… 2024
- [SoK: All You Need to Know About On-Device ML Model Extraction](https://www.usenix.org/system/files/usenixsecurity24-nayan.pdf)

### ðŸ“… 2023
- [Survey of Vulnerabilities in LLMs Revealed by Adversarial Attacks](https://arxiv.org/pdf/2310.10844)
- [A Survey on Transferability of Adversarial Examples Across DNNs](https://arxiv.org/pdf/2310.17626)

---

## 4. Model Stealing and Extraction

### ðŸ“… 2024
<!-- Add new papers here -->

### ðŸ“… 2023
<!-- Add new papers here -->

### ðŸ“… Prior
- **2022**: [Surrogate-Based Black-Box Attacks on Deep Networks](https://arxiv.org/pdf/2203.08725)
- **2021**: [Meta-Surrogate Model for Transferable Adversarial Attack](https://arxiv.org/pdf/2109.01983)

---

## 5. General Adversarial Machine Learning (AML)

### ðŸ“… 2024
- [Benchmarking Transferable Adversarial Attacks](https://arxiv.org/pdf/2402.00418)

### ðŸ“… 2023
<!-- Add papers here -->

### ðŸ“… Prior
- **NeurIPS 2019**: [Adversarial Examples are not Bugs, They are Features](https://proceedings.neurips.cc/paper_files/paper/2019/file/e2c420d928d4bf8ce0ff2ec19b371514-Paper.pdf)
- **CVPR 2019**: [Feature Space Perturbations Yield More Transferable Adversarial Examples](https://openaccess.thecvf.com/content_CVPR_2019/papers/Inkawhich_Feature_Space_Perturbations_Yield_More_Transferable_Adversarial_Examples_CVPR_2019_paper.pdf)

---

